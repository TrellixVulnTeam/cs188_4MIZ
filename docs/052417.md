# Exploring Different Machine Learning Models

In our past experiments, we've been using the [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model for machine learning. However, [scikit-learn](http://scikit-learn.org/stable/index.html) offers a variety of classifiers which can potentially yield a higher ROC AUC score. We also explored two neural network model's offered by scikit-learn and [Keras/Tensorflow](https://keras.io/). 

## The Models 

* [Gradient Boosting Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
* [Extra Trees Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)
* [Bagging Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)
* [Adaboost Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
* [Support Vector Classifier](http://scikit-learn.org/stable/modules/svm.html#svm)
* [Multi-layer Perceptron Classifier](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)
* [2D-Convolutional Neural Network Classifier (Keras/TensorFlow)](https://keras.io/layers/convolutional/)

## The Code 

We refactored our script such that the model we are testing is passed in as an argument to an overall `runmodel` function. We also added leave-one-out cross validation to our script. For every run, a single patient was left out from training and used for testing. The overall cross validated score was calculated from the average. 

Each model is stored in it's own variable like so: 

```python
model1 = RandomForestClassifier(n_estimators=10)
model2 = GradientBoostingClassifier()
model3 = AdaBoostClassifier()
model4 = ExtraTreesClassifier()
model5 = BaggingClassifier()
model6 = SVC()
``` 

```python

def crossvalidate(*args, **kwargs):
    scores = []
    j = 0
    for i, _ in enumerate(data):
        if i in good_patients:

            if 'silent' in kwargs:
                if kwargs['silent']:
                    pass
                else:
                    print "real patient index:", i
            else:
                print "real patient index:", i

            kwargs['patient_index'] = j
            score, reconstruction = runmodel(*args, **kwargs)
            scores.append(score)

    cvmodel = args[0].__class__.__name__
    print "{} overall cross validated score {}".format(cvmodel, np.mean(scores))
    return np.mean(scores)

```

In `crossvalidate`, the `*args` argument is used to pass in the variable representing the model. `**kwargs` takes in a list of keyword arguments that affect the settings for each run. For now, we support the following keyword arguments: 

* patient_index: The index of the patient that is being left out in the cross validation. 
* silent: If true, only returns the overall cross-validated score. If false, returns the score of each x/12 patients. 
* quiet: If true, only returns score print statements. If false, outputs every debug and print statement. 
* print_example_vector: If true, prints out an example feature vector we used for debugging. 

The  `crossvalidate` function calls `runmodel` for every cross validation. `runmodel` receives the keyword arguments listed in `**kwargs` and sets them accordingly. Default values are set in `runmodel` if no keyword is specified.  

```python
def runmodel(model, patient_index=-1, quiet=False, silent=False, print_example_vector=False):
    """
    Runs a single classifier through sample classifications.
    """

    modelname = model.__class__.__name__

    if silent:
        quiet = True
    if not quiet:
        print "{model}: patient {patient}/{n_patients}: making training data".format(model=modelname, n_patients=n_patients, patient=patient_index+1)

    masks = get_masks(data)

    if patient_index == -1:
        patient_index = n_patients - 1

    X_train = []
    y_train = []

    # Create training data

    for i in range(n_patients):

        if i == patient_index:
            continue

        else:
            X_train_single, y_train_single = create_dataset(t2[i], masks[i], n, prune=prune)

            X_train += X_train_single
            y_train += y_train_single

    if print_example_vector:
        ex_vec = X_train[0]
        print "length of example vector:", len(ex_vec)
        print ex_vec

    # Create testing data

    if not quiet:
        print "{model}: patient {patient}/{n_patients}: making testing data".format(model=modelname, n_patients=n_patients, patient=patient_index+1)

    test_mask = masks[patient_index]
    test_t2 = t2[patient_index]

   
    X_test, y_test = create_dataset(test_t2, test_mask, n, prune=prune)

    # Train, predict, and score

    if not quiet:
        print "{model}: patient {patient}/{n_patients}: training using {n_pts} points".format(model=modelname, n_patients=n_patients, patient=patient_index+1, n_pts=len(y_train))
    model.fit(X_train, y_train)

    if not quiet:
        print "{model}: patient {patient}/{n_patients}: testing using {n_pts} points".format(model=modelname, n_patients=n_patients, patient=patient_index+1, n_pts=len(y_test))
    y_pred = model.predict(X_test)

    score = roc_auc_score(y_true=y_test, y_score=y_pred)

    if not silent:
        print "{model}: patient {patient}/{n_patients}: score {score}".format(model=modelname, n_patients=n_patients, score=score, patient=patient_index+1)

    return score
```

## Neural Network Code

We wrote a separate script called [`neural_network.py`](https://github.com/ardunn/cs188/blob/master/pyprostate/neural_network.py). 

This was an attempt at using a basic convolutional neural network for image processing of the data. However, this
approach was not particularly useful because the images could not be fed in as a whole, since each pixel needed to be
classified in itself.

The neural network was constructed using the Keras NN library with a TensorFlow backend. 
Also, due to the large time investment and difficulty of training a successful convolutional neural network, not
as much work was put into this approach as out ensemble methods. 

The majority of the code for the convolutional neural net is taken off of one of Keras' basic examples for creating a 
neural network. The next students working this project can take the time into customizing it and exceeding 
the performance of the ensemble methods we found.

## And the winner is... 

### Model Comparison at `n = 3`

| Model                                                      | Cross Validated ROC AUC Score |
|:----------------------------------------------------------:|:-----------------------------:|
| Gradient Boosting Classifier (sklearn)                     | 0.6670                        |
| Extra Trees Classifier (sklearn)                           | 0.6271                        |
| Bagging Classifier (sklearn)                               | 0.6252                        |
| Random Forest Classifier (sklearn)                         | 0.6198                        |
| Adaboost Classifier (sklearn)                              | 0.6151                        |
| Support Vector Classification (sklearn)                    | 0.5001                        |
| Multi-layer Perceptron Classifier (sklearn)                | 0.4998                        |
| Convolutional Neural Network Classifier (keras/tensorflow) | 0.4995                        |


The scores of each model are displayed in descending order. With the Gradient Boosting Classifier scoring the highest, we've decided to move forward with this model. Our next goal is to refine the GBC model with normalization and filtering.

[Back](./) 
